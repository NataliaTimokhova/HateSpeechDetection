{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e76a0e7-e90e-457e-8dc4-81cc472f2f1e",
   "metadata": {},
   "source": [
    "The goal of this project is to use the pretrained RoBERTa transformer as a feature extractor with a costum classification head to determine if text messages are offensive or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cfbb6aaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel, AutoModelForSequenceClassification, AutoConfig\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import wandb\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add src/ to path (once, so imports work)\n",
    "sys.path.append(str(Path().resolve().parent / \"src\"))\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from helper_functions import AttentionPooling, HateSpeechDataset\n",
    "from helper_functions import train_model, test_model, get_class_distribution, oversample_dataset, undersample_dataset\n",
    "from models import CustomClassifier, LargeCustomClassifier, BaseCNNClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "33448265-13a5-44f5-9ba4-0fb9cc41448c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned data path: /Project/data/cleaned\n",
      "Processed data path: /Project/data/processed\n"
     ]
    }
   ],
   "source": [
    "from paths import DATA_CLEANED, DATA_PROCESSED\n",
    "print(\"Cleaned data path:\", DATA_CLEANED)\n",
    "print(\"Processed data path:\", DATA_PROCESSED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f3f24224-26ee-49a1-adf0-48e3e6d38544",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# Use GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f040a974",
   "metadata": {},
   "source": [
    "## Using RoBERTa as a feature extractor with a costum classification head"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ee4da52",
   "metadata": {},
   "source": [
    "Found this pretrained model online: cardiffnlp/twitter-roberta-base-sentiment-latest (https://huggingface.co/cardiffnlp/twitter-roberta-base-sentiment-latest)\n",
    "\n",
    "It is already pretrained on twitter messages. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc731c7",
   "metadata": {},
   "source": [
    "Define which pretrained model is used and initilise tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "669dc7c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'roberta-base'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20bb86e8",
   "metadata": {},
   "source": [
    "## Load HASOC dataset for training, validation and testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0181465c-ef90-409f-9cb4-39f7db6f2e93",
   "metadata": {},
   "source": [
    "Define experiment scope:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "479a3f53-d232-42b2-8dd3-169bce3efd4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we are just using the labels of the first task of the HASOC dataset, which is a binary classification\n",
    "label = \"task_1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2f76fc4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training and test data\n",
    "clean_df = pd.read_csv(DATA_CLEANED / \"hasoc_2019_en_train_cleaned.tsv\", sep='\\t')\n",
    "# test_df = pd.read_csv(DATA_PROCESSED / \"hasoc_2019_en_test.tsv\", sep='\\t')\n",
    "test_df = pd.read_csv(DATA_CLEANED / \"hasoc_2019_en_test_cleaned.tsv\", sep='\\t')\n",
    "\n",
    "# Split clean dataset in training and validation set\n",
    "train_df, val_df = train_test_split(clean_df, test_size=0.3, random_state=42, stratify=clean_df[label])\n",
    "\n",
    "# Automatically map string labels to integers\n",
    "label_list = sorted(train_df[label].unique())\n",
    "label_map = {label: idx for idx, label in enumerate(label_list)}\n",
    "\n",
    "train_df[label] = train_df[label].map(label_map)\n",
    "val_df[label] = val_df[label].map(label_map)\n",
    "test_df[label] = test_df[label].map(label_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e9fb66be-035e-46e1-9e43-5de03957c87a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decide which technique to use to cope with data imbalance\n",
    "handling_imbalance = \"class_weighting\"\n",
    "# when choosing 'class_weighting' dataset is not touched but classes gets weighted depending on label/class distribution\n",
    "\n",
    "if handling_imbalance == 'oversampling':\n",
    "    # Oversample dataset\n",
    "    train_df = oversample_dataset(train_df, label)\n",
    "    # val_df = oversample_dataset(val_df, label) # over and undersampling only useful for training dataset\n",
    "    # test_df = oversample_dataset(test_df, label)\n",
    "elif handling_imbalance == 'undersampling':\n",
    "    # Undersample dataset\n",
    "    train_df = undersample_dataset(train_df, label)\n",
    "    # val_df = undersample_dataset(val_df, label)\n",
    "    # test_df = undersample_dataset(test_df, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1ae20628-d358-4fe0-95fd-fb79e925436a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 2513, 0: 1583}\n"
     ]
    }
   ],
   "source": [
    "# Create PyTorch Datasets and DataLoaders\n",
    "train_dataset = HateSpeechDataset(train_df, tokenizer, label=label)\n",
    "val_dataset = HateSpeechDataset(val_df, tokenizer, label=label)\n",
    "test_dataset = HateSpeechDataset(test_df, tokenizer, label=label)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32)\n",
    "\n",
    "print(get_class_distribution(train_df, label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "197592e6-249f-4274-97a2-ca3625e089f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weights = compute_class_weight(\n",
    "    class_weight=\"balanced\",\n",
    "    classes=np.unique(train_df[label]),\n",
    "    y=train_df[label]\n",
    ")\n",
    "\n",
    "class_weights_tensor = torch.tensor(class_weights, dtype=torch.float).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fd402e6",
   "metadata": {},
   "source": [
    "## Training and evaluation of model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "456ec3ec-d7e1-47ad-a9f1-57dac6011ebd",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'BaseCNNClassifier' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Initialize model\u001b[39;00m\n\u001b[32m      5\u001b[39m unfrozen_last_layers = \u001b[32m2\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m model = \u001b[43mBaseCNNClassifier\u001b[49m(model_name, class_weights_tensor, device, unfrozen_last_layers).to(device)\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# Set learning rate\u001b[39;00m\n\u001b[32m     10\u001b[39m learning_rate = \u001b[32m5e-4\u001b[39m\n",
      "\u001b[31mNameError\u001b[39m: name 'BaseCNNClassifier' is not defined"
     ]
    }
   ],
   "source": [
    "# Decide what pooling to use (cls, mean or attention_pooling)\n",
    "pooling = \"max\"\n",
    "\n",
    "# Initialize model\n",
    "unfrozen_last_layers = 2\n",
    "model = BaseCNNClassifier(model_name, class_weights_tensor, device, unfrozen_last_layers).to(device)\n",
    "\n",
    "\n",
    "# Set learning rate\n",
    "learning_rate = 5e-4\n",
    "\n",
    "# Optimizer only for the classification head\n",
    "optimizer = torch.optim.AdamW(model.classifier.parameters(), lr=learning_rate)\n",
    "epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c5445349-d36a-4138-bc0f-f2dd004fe549",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">genial-pine-51</strong> at: <a href='https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-classifier/runs/chacrjdw' target=\"_blank\">https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-classifier/runs/chacrjdw</a><br> View project at: <a href='https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-classifier' target=\"_blank\">https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-classifier</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250516_000343-chacrjdw/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Project/notebooks/wandb/run-20250516_000409-snubig4y</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-classifier/runs/snubig4y' target=\"_blank\">young-meadow-52</a></strong> to <a href='https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-classifier' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-classifier' target=\"_blank\">https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-classifier</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-classifier/runs/snubig4y' target=\"_blank\">https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-classifier/runs/snubig4y</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-classifier/runs/snubig4y?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7f1cd73f4110>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init(project=\"roberta-classifier\", config={\n",
    "    \"model\": model_name,\n",
    "    \"frozen_base\": True if unfrozen_last_layers == 0 else f\"last {unfrozen_last_layers} layers unfrozen\",\n",
    "    \"pooling\": pooling,\n",
    "    \"classifier_head\": model.__class__.__name__,\n",
    "    \"epochs\": epochs,\n",
    "    \"lr\": learning_rate,\n",
    "    \"handling_imbalance\": handling_imbalance\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "781a9824-c9d1-44a9-ae3a-70399e220e96",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mtrain_model\u001b[49m(model, train_loader, val_loader, optimizer, device, epochs, \u001b[33m'\u001b[39m\u001b[33mbest_model_variants.pt\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'train_model' is not defined"
     ]
    }
   ],
   "source": [
    "train_model(model, train_loader, val_loader, optimizer, device, epochs, 'best_model_variants.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "198b4b0a",
   "metadata": {},
   "source": [
    "## Testing of model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c821845d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Load best model\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mmodel\u001b[49m.load_state_dict(torch.load(\u001b[33m\"\u001b[39m\u001b[33mbest_model_variants.pt\u001b[39m\u001b[33m\"\u001b[39m, weights_only=\u001b[38;5;28;01mTrue\u001b[39;00m))\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Test model on test set\u001b[39;00m\n\u001b[32m      4\u001b[39m test_model(model, test_loader, device, phase = \u001b[33m\"\u001b[39m\u001b[33mtest\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "# Load best model\n",
    "model.load_state_dict(torch.load(\"best_model_variants.pt\", weights_only=True))\n",
    "# Test model on test set\n",
    "test_model(model, test_loader, device, phase = \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb1a15d-c616-4a6d-bf68-c39a2c924782",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf08b446-bf6f-4c2f-9b11-e6cc2c8079ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d72ac6-c270-46aa-8033-f0a3de567da2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
