{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e76a0e7-e90e-457e-8dc4-81cc472f2f1e",
   "metadata": {},
   "source": [
    "The goal of this project is to use the pretrained RoBERTa transformer as a feature extractor with a costum classification head to determine if text messages are offensive or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cfbb6aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel, AutoModelForSequenceClassification, AutoConfig\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import wandb\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add src/ to path (once, so imports work)\n",
    "sys.path.append(str(Path().resolve().parent / \"src\"))\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from helper_functions import AttentionPooling, HateSpeechDataset\n",
    "from helper_functions import train_model, test_model, unfreeze_last_n_layers, get_class_distribution, oversample_dataset, undersample_dataset\n",
    "from models import CustomClassifier, LargeCustomClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33448265-13a5-44f5-9ba4-0fb9cc41448c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned data path: /Project/data/cleaned\n",
      "Processed data path: /Project/data/processed\n"
     ]
    }
   ],
   "source": [
    "from paths import DATA_CLEANED, DATA_PROCESSED\n",
    "print(\"Cleaned data path:\", DATA_CLEANED)\n",
    "print(\"Processed data path:\", DATA_PROCESSED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f3f24224-26ee-49a1-adf0-48e3e6d38544",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# Use GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f040a974",
   "metadata": {},
   "source": [
    "## Using RoBERTa as a feature extractor with a costum classification head"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ee4da52",
   "metadata": {},
   "source": [
    "Found this pretrained model online: cardiffnlp/twitter-roberta-base-sentiment-latest (https://huggingface.co/cardiffnlp/twitter-roberta-base-sentiment-latest)\n",
    "\n",
    "It is already pretrained on twitter messages. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc731c7",
   "metadata": {},
   "source": [
    "Define which pretrained model is used and initilise tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "669dc7c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'roberta-base'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20bb86e8",
   "metadata": {},
   "source": [
    "## Load HASOC dataset for training, validation and testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0181465c-ef90-409f-9cb4-39f7db6f2e93",
   "metadata": {},
   "source": [
    "Define experiment scope:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "479a3f53-d232-42b2-8dd3-169bce3efd4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we are just using the labels of the first task of the HASOC dataset, which is a binary classification\n",
    "label = \"task_1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2f76fc4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training and test data\n",
    "clean_df = pd.read_csv(DATA_CLEANED / \"hasoc_2019_en_train_cleaned.tsv\", sep='\\t')\n",
    "# test_df = pd.read_csv(DATA_PROCESSED / \"hasoc_2019_en_test.tsv\", sep='\\t')\n",
    "test_df = pd.read_csv(DATA_CLEANED / \"hasoc_2019_en_test_cleaned.tsv\", sep='\\t')\n",
    "\n",
    "# Split clean dataset in training and validation set\n",
    "train_df, val_df = train_test_split(clean_df, test_size=0.3, random_state=42, stratify=clean_df[label])\n",
    "\n",
    "# Automatically map string labels to integers\n",
    "label_list = sorted(train_df[label].unique())\n",
    "label_map = {label: idx for idx, label in enumerate(label_list)}\n",
    "\n",
    "train_df[label] = train_df[label].map(label_map)\n",
    "val_df[label] = val_df[label].map(label_map)\n",
    "test_df[label] = test_df[label].map(label_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e9fb66be-035e-46e1-9e43-5de03957c87a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decide which technique to use to cope with data imbalance\n",
    "handling_imbalance = \"class_weighting\"\n",
    "# when choosing 'class_weighting' dataset is not touched but classes gets weighted depending on label/class distribution\n",
    "\n",
    "if handling_imbalance == 'oversampling':\n",
    "    # Oversample dataset\n",
    "    train_df = oversample_dataset(train_df, label)\n",
    "    # val_df = oversample_dataset(val_df, label) # over and undersampling only useful for training dataset\n",
    "    # test_df = oversample_dataset(test_df, label)\n",
    "elif handling_imbalance == 'undersampling':\n",
    "    # Undersample dataset\n",
    "    train_df = undersample_dataset(train_df, label)\n",
    "    # val_df = undersample_dataset(val_df, label)\n",
    "    # test_df = undersample_dataset(test_df, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1ae20628-d358-4fe0-95fd-fb79e925436a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 2513, 0: 1583}\n"
     ]
    }
   ],
   "source": [
    "# Create PyTorch Datasets and DataLoaders\n",
    "train_dataset = HateSpeechDataset(train_df, tokenizer, label=label)\n",
    "val_dataset = HateSpeechDataset(val_df, tokenizer, label=label)\n",
    "test_dataset = HateSpeechDataset(test_df, tokenizer, label=label)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32)\n",
    "\n",
    "print(get_class_distribution(train_df, label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "197592e6-249f-4274-97a2-ca3625e089f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weights = compute_class_weight(\n",
    "    class_weight=\"balanced\",\n",
    "    classes=np.unique(train_df[label]),\n",
    "    y=train_df[label]\n",
    ")\n",
    "\n",
    "class_weights_tensor = torch.tensor(class_weights, dtype=torch.float).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fd402e6",
   "metadata": {},
   "source": [
    "## Training and evaluation of model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "456ec3ec-d7e1-47ad-a9f1-57dac6011ebd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-15 15:37:17.152664: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-05-15 15:37:17.168563: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1747316237.188326   67007 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1747316237.194396   67007 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1747316237.208778   67007 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1747316237.208796   67007 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1747316237.208797   67007 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1747316237.208799   67007 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-05-15 15:37:17.214222: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Decide what pooling to use (cls, mean or attention_pooling)\n",
    "pooling = \"attention_pooling\"\n",
    "\n",
    "# Initialize model\n",
    "model = CustomClassifier(model_name, class_weights_tensor, device, pooling=pooling).to(device)\n",
    "\n",
    "# Unfreeze the last 2 layers of the transformer\n",
    "unfreeze_last_n_layers(model, n_layers=2)\n",
    "\n",
    "# Set learning rate\n",
    "learning_rate = 1e-5\n",
    "\n",
    "# Optimizer only for the classification head\n",
    "optimizer = torch.optim.AdamW(model.classifier.parameters(), lr=learning_rate)\n",
    "epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c5445349-d36a-4138-bc0f-f2dd004fe549",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mnatalia-timokhova-v\u001b[0m (\u001b[33mnatalia-timokhova-v-lule-university-of-technology\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Project/notebooks/wandb/run-20250515_153720-368jm8el</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-classifier/runs/368jm8el' target=\"_blank\">noble-terrain-44</a></strong> to <a href='https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-classifier' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-classifier' target=\"_blank\">https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-classifier</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-classifier/runs/368jm8el' target=\"_blank\">https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-classifier/runs/368jm8el</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-classifier/runs/368jm8el?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7fca35699ad0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init(project=\"roberta-classifier\", config={\n",
    "    \"model\": model_name,\n",
    "    \"frozen_base\": True,\n",
    "    \"pooling\": pooling,\n",
    "    \"classifier_head\": model.__class__.__name__,\n",
    "    \"epochs\": epochs,\n",
    "    \"lr\": learning_rate,\n",
    "    \"handling_imbalance\": handling_imbalance\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "781a9824-c9d1-44a9-ae3a-70399e220e96",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1\n",
      "Training Loss: 0.7411\n",
      "Train Accuracy: 0.6133\n",
      "Train F1 (macro): 0.3807\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.7037\n",
      "Val Accuracy: 0.6139\n",
      "Val F1 (macro): 0.3804\n",
      "\n",
      "New best model saved (F1: 0.3804, Acc: 0.6139)\n",
      "\n",
      "Epoch 2\n",
      "Training Loss: 0.6931\n",
      "Train Accuracy: 0.5994\n",
      "Train F1 (macro): 0.4526\n",
      "\n",
      "Val Loss: 0.6855\n",
      "Val Accuracy: 0.6173\n",
      "Val F1 (macro): 0.5106\n",
      "\n",
      "New best model saved (F1: 0.5106, Acc: 0.6173)\n",
      "\n",
      "Epoch 3\n"
     ]
    }
   ],
   "source": [
    "train_model(model, train_loader, val_loader, optimizer, device, epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "198b4b0a",
   "metadata": {},
   "source": [
    "## Testing of model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c821845d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model\n",
    "model.load_state_dict(torch.load(\"best_model.pt\", weights_only=True))\n",
    "# Test model on test set\n",
    "test_model(model, test_loader, device, phase = \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb1a15d-c616-4a6d-bf68-c39a2c924782",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "adl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
