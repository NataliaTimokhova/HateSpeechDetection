{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e76a0e7-e90e-457e-8dc4-81cc472f2f1e",
   "metadata": {},
   "source": [
    "The goal of this project is to use the pretrained RoBERTa transformer as a feature extractor with a costum classification head to determine if text messages are offensive or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cfbb6aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel, AutoModelForSequenceClassification, AutoConfig\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import wandb\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add src/ to path (once, so imports work)\n",
    "sys.path.append(str(Path().resolve().parent / \"src\"))\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from helper_functions import train_model, test_model, get_class_distribution, oversample_dataset, undersample_dataset, AttentionPooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33448265-13a5-44f5-9ba4-0fb9cc41448c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned data path: /Project/data/cleaned\n",
      "Processed data path: /Project/data/processed\n"
     ]
    }
   ],
   "source": [
    "from paths import DATA_CLEANED, DATA_PROCESSED\n",
    "print(\"Cleaned data path:\", DATA_CLEANED)\n",
    "print(\"Processed data path:\", DATA_PROCESSED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f3f24224-26ee-49a1-adf0-48e3e6d38544",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# Use GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f040a974",
   "metadata": {},
   "source": [
    "## Using RoBERTa as a feature extractor with a costum classification head"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ee4da52",
   "metadata": {},
   "source": [
    "Found this pretrained model online: cardiffnlp/twitter-roberta-base-sentiment-latest (https://huggingface.co/cardiffnlp/twitter-roberta-base-sentiment-latest)\n",
    "\n",
    "It is already pretrained on twitter messages. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc731c7",
   "metadata": {},
   "source": [
    "Define which pretrained model is used and initilise tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "669dc7c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'roberta-base'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce20ed5c",
   "metadata": {},
   "source": [
    "Classification head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "83da8799-f10a-4fbe-ba78-cb866aaf4811",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomClassifier(nn.Module):\n",
    "    def __init__(self, model_name, class_weights_tensor, pooling='cls'):\n",
    "        super().__init__()\n",
    "        self.class_weights = class_weights_tensor\n",
    "        self.pooling = pooling.lower()\n",
    "        self.base = AutoModel.from_pretrained(model_name)\n",
    "        config = AutoConfig.from_pretrained(model_name)\n",
    "        hidden_size = config.hidden_size  # Dynamically get the model's hidden size\n",
    "\n",
    "        if pooling == 'attention_pooling':\n",
    "            # Replace CLS pooling with attention\n",
    "            self.attention_pool = AttentionPooling(hidden_size)\n",
    "        \n",
    "        # Freeze all parameters of the base model\n",
    "        for param in self.base.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # Custom classification head\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_size, 64),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(64, 2)\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, class_weights_tensor = None, labels=None):\n",
    "        outputs = self.base(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "        # Pooling strategy: either CLS token or mean pooling over token embeddings\n",
    "        if self.pooling == 'mean':\n",
    "            token_embeddings = outputs.last_hidden_state\n",
    "            input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size())\n",
    "            sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\n",
    "            sum_mask = input_mask_expanded.sum(1).clamp(min=1e-9)\n",
    "            pooled = sum_embeddings / sum_mask\n",
    "        elif self.pooling == 'attention_pooling':\n",
    "            pooled = self.attention_pool(outputs.last_hidden_state, attention_mask)\n",
    "        else:\n",
    "            pooled = outputs.last_hidden_state[:, 0, :]  # CLS token\n",
    "\n",
    "        logits = self.classifier(pooled)\n",
    "\n",
    "        # If labels are provided, calculate the loss\n",
    "        if labels is not None:\n",
    "            loss_fn = nn.CrossEntropyLoss(weight=class_weights_tensor.to(device))\n",
    "            loss = loss_fn(logits, labels)\n",
    "            return logits, loss\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20bb86e8",
   "metadata": {},
   "source": [
    "## Load HASOC dataset for training, validation and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f133729a-b5db-4e99-809d-12a6ba529ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Dataset Class\n",
    "class HateSpeechDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, label = 'label', max_len=128):\n",
    "        self.texts = df[\"text\"].tolist()\n",
    "        self.labels = df[label].tolist()\n",
    "        self.encodings = tokenizer(self.texts, padding=True, truncation=True, max_length=max_len)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0181465c-ef90-409f-9cb4-39f7db6f2e93",
   "metadata": {},
   "source": [
    "Define experiment scope:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "479a3f53-d232-42b2-8dd3-169bce3efd4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we are just using the labels of the first task of the HASOC dataset, which is a binary classification\n",
    "label = \"task_1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2f76fc4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training and test data\n",
    "clean_df = pd.read_csv(DATA_CLEANED / \"hasoc_2019_en_train_cleaned.tsv\", sep='\\t')\n",
    "# test_df = pd.read_csv(DATA_PROCESSED / \"hasoc_2019_en_test.tsv\", sep='\\t')\n",
    "test_df = pd.read_csv(DATA_CLEANED / \"hasoc_2019_en_test_cleaned.tsv\", sep='\\t')\n",
    "\n",
    "# Split clean dataset in training and validation set\n",
    "train_df, val_df = train_test_split(clean_df, test_size=0.3, random_state=42, stratify=clean_df[label])\n",
    "\n",
    "# Automatically map string labels to integers\n",
    "label_list = sorted(train_df[label].unique())\n",
    "label_map = {label: idx for idx, label in enumerate(label_list)}\n",
    "\n",
    "train_df[label] = train_df[label].map(label_map)\n",
    "val_df[label] = val_df[label].map(label_map)\n",
    "test_df[label] = test_df[label].map(label_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e9fb66be-035e-46e1-9e43-5de03957c87a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decide which technique to use to cope with data imbalance\n",
    "handling_imbalance = \"class_weighting\"\n",
    "# when choosing 'class_weighting' dataset is not touched but classes gets weighted depending on label/class distribution\n",
    "\n",
    "if handling_imbalance == 'oversampling':\n",
    "    # Oversample dataset\n",
    "    train_df = oversample_dataset(train_df, label)\n",
    "    # val_df = oversample_dataset(val_df, label) # over and undersampling only useful for training dataset\n",
    "    # test_df = oversample_dataset(test_df, label)\n",
    "elif handling_imbalance == 'undersampling':\n",
    "    # Undersample dataset\n",
    "    train_df = undersample_dataset(train_df, label)\n",
    "    # val_df = undersample_dataset(val_df, label)\n",
    "    # test_df = undersample_dataset(test_df, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1ae20628-d358-4fe0-95fd-fb79e925436a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 2513, 0: 1583}\n"
     ]
    }
   ],
   "source": [
    "# Create PyTorch Datasets and DataLoaders\n",
    "train_dataset = HateSpeechDataset(train_df, tokenizer, label=label)\n",
    "val_dataset = HateSpeechDataset(val_df, tokenizer, label=label)\n",
    "test_dataset = HateSpeechDataset(test_df, tokenizer, label=label)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32)\n",
    "\n",
    "print(get_class_distribution(train_df, label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "197592e6-249f-4274-97a2-ca3625e089f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weights = compute_class_weight(\n",
    "    class_weight=\"balanced\",\n",
    "    classes=np.unique(train_df[label]),\n",
    "    y=train_df[label]\n",
    ")\n",
    "\n",
    "class_weights_tensor = torch.tensor(class_weights, dtype=torch.float).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fd402e6",
   "metadata": {},
   "source": [
    "## Training and evaluation of model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "456ec3ec-d7e1-47ad-a9f1-57dac6011ebd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-14 13:57:45.643272: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-05-14 13:57:45.658922: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1747223865.678549   60513 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1747223865.684633   60513 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1747223865.699093   60513 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1747223865.699108   60513 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1747223865.699110   60513 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1747223865.699111   60513 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-05-14 13:57:45.704497: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Decide what pooling to use\n",
    "pooling = \"attention_pooling\"\n",
    "\n",
    "# Initialize model\n",
    "model = CustomClassifier(model_name, class_weights_tensor, pooling=pooling).to(device)\n",
    "\n",
    "# Optimizer only for the classification head\n",
    "optimizer = torch.optim.AdamW(model.classifier.parameters(), lr=2e-4)\n",
    "epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c5445349-d36a-4138-bc0f-f2dd004fe549",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mnatalia-timokhova-v\u001b[0m (\u001b[33mnatalia-timokhova-v-lule-university-of-technology\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Project/notebooks/wandb/run-20250514_135748-l3v6tm13</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-classifier/runs/l3v6tm13' target=\"_blank\">lyric-donkey-33</a></strong> to <a href='https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-classifier' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-classifier' target=\"_blank\">https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-classifier</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-classifier/runs/l3v6tm13' target=\"_blank\">https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-classifier/runs/l3v6tm13</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/natalia-timokhova-v-lule-university-of-technology/roberta-classifier/runs/l3v6tm13?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7f79f62f1610>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init(project=\"roberta-classifier\", config={\n",
    "    \"model\": model_name,\n",
    "    \"frozen_base\": True,\n",
    "    \"pooling\": pooling,\n",
    "    \"epochs\": epochs,\n",
    "    \"lr\": 1e-5,\n",
    "    \"handling_imbalance\": handling_imbalance\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "781a9824-c9d1-44a9-ae3a-70399e220e96",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1\n",
      "Training Loss: 0.6683\n",
      "Train Accuracy: 0.5886\n",
      "Train F1 (macro): 0.5758\n",
      "\n",
      "Val Loss: 0.6482\n",
      "Val Accuracy: 0.6207\n",
      "Val F1 (macro): 0.6128\n",
      "\n",
      "New best model saved (F1: 0.6128, Acc: 0.6207)\n",
      "\n",
      "Epoch 2\n",
      "Training Loss: 0.6364\n",
      "Train Accuracy: 0.6340\n",
      "Train F1 (macro): 0.6231\n",
      "\n",
      "Val Loss: 0.6366\n",
      "Val Accuracy: 0.6333\n",
      "Val F1 (macro): 0.6289\n",
      "\n",
      "New best model saved (F1: 0.6289, Acc: 0.6333)\n",
      "\n",
      "Epoch 3\n"
     ]
    }
   ],
   "source": [
    "train_model(model, train_loader, val_loader, optimizer, device, epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "198b4b0a",
   "metadata": {},
   "source": [
    "## Testing of model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c821845d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model\n",
    "model.load_state_dict(torch.load(\"best_model.pt\", weights_only=True))\n",
    "# Test model on test set\n",
    "test_model(model, test_loader, device, phase = \"test\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "adl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
